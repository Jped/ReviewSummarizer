{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written as part of https://www.scrapehero.com/how-to-scrape-amazon-product-reviews-using-python/, modified by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "from json import dump,loads\n",
    "from requests import get\n",
    "import json\n",
    "from re import sub\n",
    "from dateutil import parser as dateparser\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseReviews(amazon_url):\n",
    "    '''\n",
    "    Given a url to an amazon product, this function returns all\n",
    "    of its amazon reviews in a json format.\n",
    "    \n",
    "    Each review is made up of both some text and a rating(1-5).\n",
    "    '''\n",
    "    # Add some recent user agent to prevent amazon from blocking the request \n",
    "    # Find some chrome user agent strings  here https://udger.com/resources/ua-list/browser-detail?browser=Chrome\n",
    "    headers = {'User-Agent': '''Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'''\n",
    "                               '''(KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'''}\n",
    "    \n",
    "    # try getting the data 5 times. Will only retry getting data if response code isn't 200\n",
    "    for i in range(5): \n",
    "        # The response is the whole page (html, css, javascript, response code)\n",
    "        response = get(amazon_url, headers = headers, timeout=30)\n",
    "        if response.status_code == 404:\n",
    "            return {\"url\": amazon_url, \"error\": \"page not found\"}\n",
    "        if response.status_code != 200: # checks whether to retry getting the page.\n",
    "            continue\n",
    "        \n",
    "        # Removing the null bytes from the response. \n",
    "        cleaned_response = response.text.replace('\\x00', '') \n",
    "\n",
    "        # get html in tree structure that can be parsed with XPath\n",
    "        parser = html.fromstring(cleaned_response) \n",
    "        \n",
    "        XPATH_AGGREGATE = '//span[@id=\"acrCustomerReviewText\"]'\n",
    "        XPATH_REVIEW_SECTION_1 = '//div[contains(@id,\"reviews-summary\")]'\n",
    "        XPATH_REVIEW_SECTION_2 = '//div[@data-hook=\"review\"]'\n",
    "        \n",
    "        reviews = parser.xpath(XPATH_REVIEW_SECTION_1)\n",
    "        \n",
    "        if not reviews:\n",
    "            reviews = parser.xpath(XPATH_REVIEW_SECTION_2)\n",
    "        \n",
    "        reviews_list = []\n",
    "\n",
    "        \n",
    "        # Parsing individual reviews\n",
    "        for review in reviews:\n",
    "            \n",
    "            XPATH_RATING  = './/i[@data-hook=\"review-star-rating\"]//text()'\n",
    "            raw_review_rating = review.xpath(XPATH_RATING)\n",
    "            review_rating = ''.join(raw_review_rating).replace('out of 5 stars', '')\n",
    "\n",
    "            XPATH_REVIEW_TEXT = './/span[@data-hook=\"review-body\"]//text()'\n",
    "            raw_review_text = review.xpath(XPATH_REVIEW_TEXT)\n",
    "            review_text = ' '.join(' '.join(raw_review_text).split())\n",
    "            \n",
    "            reviews_list.append({'review_text': review_text, 'review_rating': review_rating})\n",
    "\n",
    "        data = { 'url': amazon_url,\n",
    "                 'reviews': reviews_list }\n",
    "        \n",
    "        return data\n",
    "\n",
    "    return {\"error\": \"failed to process the page\", \"url\": amazon_url}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalReviewcount(amazon_url):\n",
    "    '''\n",
    "    Given an amazon product url, this function scrapes the total\n",
    "    amount of reviews that the product has, and returns the value.\n",
    "    '''\n",
    "    # Add some recent user agent to prevent amazon from blocking the request \n",
    "    # Find some chrome user agent strings  here https://udger.com/resources/ua-list/browser-detail?browser=Chrome\n",
    "    headers = {'User-Agent': '''Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'''\n",
    "                               '''(KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'''}\n",
    "    \n",
    "    # try getting the data 5 times. Will only retry getting data if response code isn't 200\n",
    "    for i in range(5): \n",
    "        # The response is the whole page (html, css, javascript, response code)\n",
    "        response = get(amazon_url, headers = headers, timeout=30)\n",
    "        if response.status_code == 404:\n",
    "            return {\"url\": amazon_url, \"error\": \"page not found\"}\n",
    "        if response.status_code != 200: # checks whether to retry getting the page.\n",
    "            continue\n",
    "        \n",
    "        # Removing the null bytes from the response. \n",
    "        cleaned_response = response.text.replace('\\x00', '') \n",
    "\n",
    "        # get html in tree structure that can be parsed with XPath\n",
    "        parser = html.fromstring(cleaned_response) \n",
    "        \n",
    "        # getting the product name\n",
    "        XPATH_PRODUCT_NAME = '//h1//span[@id=\"productTitle\"]//text()'\n",
    "        raw_product_name = parser.xpath(XPATH_PRODUCT_NAME)\n",
    "        product_name = ''.join(raw_product_name).strip()\n",
    "        \n",
    "        # getting the total review count\n",
    "        XPATH_REVIEW_COUNT = '//h3//span[@data-hook=\"top-customer-reviews-title\"]//text()'\n",
    "        raw_review_count = parser.xpath(XPATH_REVIEW_COUNT)\n",
    "        review_count = ''.join(raw_review_count).strip()\n",
    "\n",
    "        starting_index = review_count.find('of ') + 3\n",
    "        ending_index = review_count.find(' reviews')\n",
    "        review_count = review_count[starting_index:ending_index]\n",
    "        \n",
    "        review_count = review_count.replace(',', '') # get rid of any potential commas\n",
    "        \n",
    "        review_count = int(review_count)\n",
    "\n",
    "        # getting the aggregated ratings\n",
    "        XPATH_AGGREGATE_RATING = '//table[@id=\"histogramTable\"]//tr'\n",
    "        total_ratings  = parser.xpath(XPATH_AGGREGATE_RATING)\n",
    "     \n",
    "        ratings_dict = {}\n",
    "        for ratings in total_ratings:\n",
    "            extracted_rating = ratings.xpath('./td//a//text()')\n",
    "            if extracted_rating:\n",
    "                rating_key = extracted_rating[0] \n",
    "                rating_value = extracted_rating[1]\n",
    "                if rating_key:\n",
    "                    ratings_dict.update({rating_key: rating_value})\n",
    "        \n",
    "        return { 'url': amazon_url,\n",
    "                 'name': product_name,\n",
    "                 'review-count': review_count,\n",
    "                 'ratings': ratings_dict }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeAmazonReviews(output_folder = None):\n",
    "    '''\n",
    "    Scrapes the user-inputted website for reviews, writing them\n",
    "    into a json file.\n",
    "\n",
    "    If a global path to an output folder is not inputted, this\n",
    "    function defaults to writing the json file into ./scrapped-reviews/.\n",
    "    '''\n",
    "    if not output_folder:\n",
    "        output_folder = './scrapped-reviews/'\n",
    "    \n",
    "    link = input('Website link:')\n",
    "    \n",
    "    # get the name, the amount of reviews, and the aggregated ratings\n",
    "    basic_info = getTotalReviewcount(link)\n",
    "    \n",
    "    if len(basic_info['name']) > 15:\n",
    "        f = open(output_folder + basic_info['name'][:15] + '..._product_reviews.json', 'w')\n",
    "    else:\n",
    "        f = open(output_folder + basic_info['name'][:15] + '_product_reviews.json', 'w')    \n",
    "            \n",
    "    dump(basic_info, f, indent=4)\n",
    "\n",
    "    # get the reviews\n",
    "    reviews_link = link.replace('/dp/', '/product-reviews/')\n",
    "    \n",
    "    last_index1 = reviews_link.find('?')\n",
    "    last_index2 = reviews_link.find('/ref')\n",
    "    if last_index1 is -1:\n",
    "        min_index = last_index2\n",
    "    elif last_index2 is -1:\n",
    "        min_index = last_index1\n",
    "    else:\n",
    "        min_index = min(last_index1, last_index2)\n",
    "        \n",
    "    reviews_link = reviews_link[:min_index]\n",
    "    reviews_link = reviews_link + '/?pageNumber='\n",
    "    \n",
    "    for i in range(1, (basic_info['review-count']//10) + 2):\n",
    "        extracted_data = ParseReviews(reviews_link + str(i))\n",
    "        dump(extracted_data, f, indent=4)\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website link:https://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871/ref=pd_rhf_dp_s_pd_crcd_0_2?_encoding=UTF8&pd_rd_i=0262510871&pd_rd_r=f23b48a0-5b97-473c-8cdd-805e0d59bf5c&pd_rd_w=VxKQA&pd_rd_wg=Ulq03&pf_rd_p=c6269878-d677-4a89-a68c-ff0df2b6ce6c&pf_rd_r=J0DZWERF1HJTT166H6D1&psc=1&refRID=J0DZWERF1HJTT166H6D1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    scrapeAmazonReviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function scrapeAmazonReviews in module __main__:\n",
      "\n",
      "scrapeAmazonReviews(output_folder=None)\n",
      "    Scrapes the user-inputted website for reviews, writing them\n",
      "    into a json file.\n",
      "    \n",
      "    If a global path to an output folder is not inputted, this\n",
      "    function defaults to writing the json file into ./scrapped-reviews/.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(scrapeAmazonReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
